# 58同城职位信息爬虫

一个功能强大的58同城招聘信息爬虫工具，支持多城市、多页面批量抓取，具备智能数据清洗和实时保存功能。

## 🚀 功能特性

### 核心功能
- **多城市支持**：支持北京、上海、广州、深圳、成都、西安、郑州等7个主要城市
- **批量抓取**：每个城市自动抓取前5页职位信息
- **实时保存**：每抓取一个职位立即保存到Excel和JSON文件
- **智能去重**：自动过滤重复职位信息
- **数据清洗**：智能清理无效和不规范的数据

## 数据字段

本工具抓取的职位信息包含以下字段：

### 企业信息
- 企业名称
- 企业类型（智能标准化）
- 社会信用码
- 企业规模（智能标准化）
- 注册资本(万)
- 所属区域（智能清洗后的标准格式）
- 联系人
- 联系方式
- 联系邮箱
- 办公地址
- 企业简介
- 营业执照
- 企业相册

### 职位信息
- 岗位名称
- 薪资类型
- 薪资范围起
- 薪资范围至
- 工作地点
- 岗位要求
- 学历要求
- 招聘人数
- 发布时间
- 结束时间
- 工作职责
- 任职要求

## 字段匹配规则详解

### 薪资信息提取
**匹配模式：**
- `(\d+)[-~](\d+)元/月` - 月薪范围
- `(\d+)[-~](\d+)万/年` - 年薪范围
- `(\d+)[-~](\d+)千/月` - 千元月薪
- `薪资.*?(\d+)[-~](\d+)` - 通用薪资模式
- `工资.*?(\d+)[-~](\d+)` - 工资关键词模式

**处理逻辑：**
- 优先从CSS选择器提取
- 如果未找到，使用正则表达式从页面前30行文本中提取
- 薪资类型：有范围值时为"非面谈"，否则为"面谈"

### 工作地点提取
**匹配模式：**
- `(北京)\s*[-\s]*([\u4e00-\u9fa5]+区)` - 北京地区
- `(上海)\s*[-\s]*([\u4e00-\u9fa5]+区)` - 上海地区
- `(广州)\s*[-\s]*([\u4e00-\u9fa5]+区)` - 广州地区
- `(深圳)\s*[-\s]*([\u4e00-\u9fa5]+区)` - 深圳地区
- `([\u4e00-\u9fa5]+市?)\s*[-\s]*([\u4e00-\u9fa5]+区)` - 通用城市区域

**输出格式：** "城市 - 区域"（如：北京 - 朝阳区）

### 学历要求提取
**匹配模式：**
- `学历要求.*?(博士|硕士|研究生|本科|大专|专科|高中|中专|初中|不限)`
- `学历.*?(博士|硕士|研究生|本科|大专|专科|高中|中专|初中|不限)`
- `(博士|硕士|研究生|本科|大专|专科)以上`
- `要求.*?(博士|硕士|研究生|本科|大专|专科|高中|中专|初中)`

**处理逻辑：**
- 初中、中专、高中统一显示为"学历不限"
- 优先从CSS选择器`.item_condition`提取
- 备用正则表达式从页面前50行提取

### 工作经验要求提取
**匹配模式：**
- `工作经验.*?(\d+)[-~](\d+)年`
- `经验.*?(\d+)[-~](\d+)年`
- `(\d+)年以上.*?经验`
- `经验.*?(\d+)年以上`
- `(无需经验|不限经验|应届毕业生|经验不限)`

**处理逻辑：**
- 优先从CSS选择器提取，排除包含"学历"和"招"的文本
- 备用正则表达式从页面前50行提取

### 招聘人数提取
**匹配模式：**
- `招聘.*?(\d+)人`
- `招.*?(\d+)人`
- `(\d+)人`

**处理逻辑：**
- 默认值为1
- 优先从CSS选择器`.item_condition`中包含"招"和"人"的文本提取
- 备用正则表达式从页面前40行提取
- 输出为数字类型

### 发布时间提取
**匹配模式：**
- `发布时间.*?(\d{4}-\d{2}-\d{2})` - 完整日期
- `(\d{4}-\d{2}-\d{2})` - 标准日期格式
- `(\d{2}-\d{2})` - 月日格式
- `(今天|昨天|前天)` - 相对时间
- `(\d+)小时前` - 小时前
- `(\d+)天前` - 天前

### 工作职责提取
**匹配模式：**
- `岗位职责[：:]?\s*(.*?)(?=任职要求|福利待遇|联系方式|$)`
- `工作职责[：:]?\s*(.*?)(?=任职要求|福利待遇|联系方式|$)`
- `工作内容[：:]?\s*(.*?)(?=任职要求|福利待遇|联系方式|$)`

**备用规则：**
- `岗位职责[：:]?\s*([\s\S]*?)(?=任职要求|职位要求|岗位要求|$)`
- `工作职责[：:]?\s*([\s\S]*?)(?=任职要求|职位要求|岗位要求|$)`
- `工作内容[：:]?\s*([\s\S]*?)(?=任职要求|职位要求|岗位要求|$)`

**处理逻辑：**
- 优先从`.des`职位描述区域提取
- 限制长度为500字符
- 去除开头的】符号

### 任职要求提取
**匹配模式：**
- `任职要求[：:]?\s*(.*?)(?=福利待遇|联系方式|$)`
- `职位要求[：:]?\s*(.*?)(?=福利待遇|联系方式|$)`
- `岗位要求[：:]?\s*(.*?)(?=福利待遇|联系方式|$)`

**备用规则：**
- `任职要求[：:]?\s*([\s\S]*?)(?=福利待遇|联系方式|$)`
- `职位要求[：:]?\s*([\s\S]*?)(?=福利待遇|联系方式|$)`
- `岗位要求[：:]?\s*([\s\S]*?)(?=福利待遇|联系方式|$)`

**处理逻辑：**
- 优先从`.des`职位描述区域提取
- 限制长度为500字符
- 去除开头的】符号

### 联系方式提取
**匹配模式：**
- `联系电话.*?(1[3-9]\d{9})` - 联系电话关键词
- `电话.*?(1[3-9]\d{9})` - 电话关键词
- `手机.*?(1[3-9]\d{9})` - 手机关键词
- `(1[3-9]\d{9})` - 通用手机号格式

### 邮箱提取
**匹配模式：**
- `([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})` - 标准邮箱格式

### 办公地址提取
**匹配模式：**
- `办公地址.*?([\u4e00-\u9fa5]+市[\u4e00-\u9fa5]+区.*?)(?=联系|电话|邮箱|$)`
- `地址.*?([\u4e00-\u9fa5]+市[\u4e00-\u9fa5]+区.*?)(?=联系|电话|邮箱|$)`
- `公司地址.*?([\u4e00-\u9fa5]+市[\u4e00-\u9fa5]+区.*?)(?=联系|电话|邮箱|$)`

**处理逻辑：**
- 限制长度为100字符
- 只保留包含市区信息的地址

### 智能数据处理

#### 所属区域智能清洗

**清洗规则：**
- 去除"总部位于"等前缀
- 统一格式为"XX省XX市XX区"或"XX市XX区"
- 过滤包含无关词汇的内容
- 如果所属区域为空，会从工作地点自动补充

**过滤的无关词汇：**
```
找工作、免费发布、登记简历、公司福利、饭补、加班补助、
交通便利、餐补、市中心区、不匹配、人公司、福利、补助、便利、
有限公司、科技有限公司、信息科技、华南地区、华北地区、华东地区、
华西地区、在华、地区、公司在
```

**匹配模式：**
- `([\u4e00-\u9fa5]{2,4}省[\u4e00-\u9fa5]{2,4}市[\u4e00-\u9fa5]{2,4}区)` - 省市区格式
- `([\u4e00-\u9fa5]{2,4}市[\u4e00-\u9fa5]{2,4}区)` - 市区格式

**处理逻辑：**
- 长度限制：≤10个字符
- 包含无关词汇时清空该字段
- 不符合标准格式时清空该字段

#### 薪资信息标准化

**支持的薪资格式：**
- 月薪："5000-8000元/月"
- 年薪："10-15万/年"
- 千元："5-8千/月"
- 面谈："薪资面谈"、"待遇面议"

**处理逻辑：**
- 自动计算薪资范围的起始和结束值
- 智能判断薪资类型（面谈/非面谈）
- 年薪自动转换为月薪（除以12）
- 千元格式自动转换为元（乘以1000）

#### 企业规模标准化

**标准化规则：**
- "1-49人" → "小型企业(1-49人)"
- "50-99人" → "小型企业(50-99人)"
- "100-499人" → "中型企业(100-499人)"
- "500-999人" → "中型企业(500-999人)"
- "1000人以上" → "大型企业(1000人以上)"

**匹配模式：**
- `(\d+)[-~](\d+)人` - 范围格式
- `(\d+)人以上` - 以上格式
- `(\d+)人以下` - 以下格式

#### 企业类型标准化

**标准化规则：**
- 互联网相关 → "互联网/通信"
- 金融相关 → "金融/投资"
- 教育相关 → "教育/培训"
- 制造相关 → "制造/生产"
- 贸易相关 → "贸易/零售"
- 服务相关 → "服务业"
- 其他 → "其他"

**关键词匹配：**
```python
# 互联网/通信
['互联网', '网络', '软件', '科技', '信息技术', 'IT', '通信', '电子商务', '游戏']

# 金融/投资
['金融', '银行', '保险', '证券', '投资', '基金', '信贷', '财务']

# 教育/培训
['教育', '培训', '学校', '大学', '学院', '幼儿园', '早教']

# 制造/生产
['制造', '生产', '工厂', '机械', '汽车', '电子', '化工', '纺织']

# 贸易/零售
['贸易', '零售', '批发', '商贸', '超市', '商场', '电商']

# 服务业
['服务', '咨询', '物流', '餐饮', '酒店', '旅游', '医疗', '房地产']
```

## 数据质量控制

### 数据过滤规则
**过滤条件（满足任一条件将被过滤）：**
- 企业名称为空或无效
- 工作职责为空
- 任职要求为空
- 岗位名称包含无关内容（如"兼职"、"代理"等）

### 数据验证规则
**必填字段验证：**
- 企业名称：长度 > 0
- 岗位名称：长度 > 0
- 工作职责：长度 > 10
- 任职要求：长度 > 10

**数据格式验证：**
- 薪资范围：必须为数字类型
- 招聘人数：必须为正整数
- 联系方式：必须符合手机号格式
- 邮箱：必须符合邮箱格式

### 重复数据处理
**去重策略：**
- 基于企业名称 + 岗位名称进行去重
- 保留最新抓取的数据
- 记录重复数据统计信息

### 异常数据处理
**异常情况处理：**
- 页面加载超时：记录日志，跳过该职位
- 验证码出现：自动刷新页面重试
- 反爬虫检测：随机延时后重试
- 数据解析失败：使用备用解析规则

## 📋 系统要求

### 环境依赖
- Python 3.7+
- Chrome浏览器
- ChromeDriver（自动管理）

### 必需库
```
selenium>=4.0.0
beautifulsoup4>=4.9.0
pandas>=1.3.0
requests>=2.25.0
webdriver-manager>=3.8.0
openpyxl>=3.0.0
```

## 🛠️ 安装与配置

### 1. 克隆项目
```bash
git clone <repository-url>
cd 58同城抓取数据
```

### 2. 创建虚拟环境
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac
```

### 3. 安装依赖
```bash
pip install -r requirements.txt
```

### 4. 运行脚本
```bash
python enhanced_job_scraper.py
```

## 📖 使用示例

### 字段匹配示例

**薪资信息提取示例：**
```
原始文本："薪资待遇：8000-12000元/月"
匹配结果：
- 薪资范围起：8000
- 薪资范围至：12000
- 薪资类型：非面谈
```

**工作地点提取示例：**
```
原始文本："工作地点：北京朝阳区"
匹配结果："北京 - 朝阳区"
```

**学历要求提取示例：**
```
原始文本："学历要求：本科及以上"
匹配结果："本科"

原始文本："高中学历即可"
匹配结果："学历不限"
```

**工作职责提取示例：**
```
原始文本："岗位职责：1.负责产品设计；2.参与需求分析"
匹配结果："1.负责产品设计；2.参与需求分析"
```

### 数据清洗示例

**所属区域清洗示例：**
```
原始数据："总部位于北京市朝阳区"
清洗结果："北京市朝阳区"

原始数据："找工作就来我们公司"
清洗结果：""（被过滤）
```

**企业类型标准化示例：**
```
原始数据："互联网科技公司"
标准化结果："互联网/通信"

原始数据："教育培训机构"
标准化结果："教育/培训"
```

## 📊 输出文件

### 主要输出
- `58同城多城市职位详细信息.xlsx` - Excel格式的职位数据（实时更新）
- `58同城多城市职位详细信息.json` - JSON格式的职位数据备份（实时更新）
- `job_scraper_YYYYMMDD_HHMMSS.log` - 详细的运行日志文件

### 日志系统
脚本内置了完善的日志记录系统：

```python
# 日志配置
def setup_logging():
    log_filename = f"job_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
```

**日志内容包括：**
- 脚本启动和结束时间
- 每个城市的抓取进度
- 每个职位的处理状态
- 数据验证和过滤结果
- 错误信息和异常处理
- 验证码检测和处理过程
- 数据保存操作记录

### 辅助工具
- `other/clean_region_enhanced.py` - 数据清洗脚本
- `other/data_comparison.py` - Excel与JSON数据一致性检查
- `other/check_json_count.py` - 数据统计工具

### 数据文件结构

#### Excel文件结构
```
| 列名 | 数据类型 | 说明 |
|------|----------|------|
| 企业名称 | 文本 | 公司名称 |
| 岗位名称 | 文本 | 职位标题 |
| 薪资类型 | 文本 | 面谈/非面谈 |
| 薪资范围起 | 数字 | 最低薪资 |
| 薪资范围至 | 数字 | 最高薪资 |
| 工作地点 | 文本 | 城市-区域格式 |
| 学历要求 | 文本 | 标准化学历 |
| 工作经验 | 文本 | 经验要求 |
| 招聘人数 | 数字 | 招聘数量 |
| 发布时间 | 文本 | 职位发布日期 |
| 工作职责 | 文本 | 岗位职责描述 |
| 任职要求 | 文本 | 任职要求描述 |
| 企业类型 | 文本 | 标准化行业类型 |
| 企业规模 | 文本 | 标准化企业规模 |
| 所属区域 | 文本 | 清洗后的地址信息 |
| 抓取城市 | 文本 | 数据来源城市 |
```

## 🔧 技术架构

### 核心类：Enhanced58JobScraper

#### 主要方法
1. **`__init__(headless=True)`** - 初始化爬虫，配置Chrome选项和浏览器设置
2. **`scrape_multiple_pages(base_url, max_pages=5)`** - 批量抓取多页数据的主控制器
3. **`generate_page_urls(base_url, max_pages=5)`** - 智能生成分页URL列表
4. **`get_job_list_from_page(url)`** - 抓取单页职位列表并处理验证码
5. **`get_job_links()`** - 从当前页面提取所有职位详情链接
6. **`scrape_job_detail_page(job_url)`** - 抓取职位详情页面信息
7. **`scrape_company_detail_page(company_url)`** - 抓取企业详情页面信息
8. **`save_single_job_to_excel(job_data, filename)`** - 实时保存单个职位到Excel和JSON
9. **`save_to_excel(data, filename)`** - 批量保存数据到Excel文件
10. **`handle_captcha(max_retries=3)`** - 智能验证码检测和处理
11. **`standardize_company_scale(scale_text)`** - 企业规模标准化处理
12. **`standardize_company_type(type_text)`** - 企业类型标准化处理
13. **`clear_excel_data(filename)`** - 清空Excel文件数据但保留表头

### 核心逻辑流程

#### 1. 初始化阶段
```
配置Chrome选项 → 设置反检测参数 → 初始化WebDriver → 配置日志系统
```

#### 2. 数据抓取流程
```
城市URL配置 → 清空历史数据 → 生成分页URL → 批量抓取页面 → 提取职位链接 → 抓取职位详情 → 数据清洗验证 → 实时保存
```

#### 3. 单页处理逻辑
```python
# 伪代码展示核心逻辑
def get_job_list_from_page(url):
    访问页面URL
    等待页面加载完成
    检测并处理验证码
    提取所有职位链接
    for 每个职位链接:
        抓取职位详情
        验证数据完整性
        实时保存到Excel和JSON
        添加延时避免频繁请求
```

#### 4. 数据验证与过滤逻辑
```python
# 数据质量控制
def save_single_job_to_excel(job_data):
    if 企业名称为空: return False
    if 工作职责为空: return False  
    if 任职要求为空: return False
    
    # 智能补充所属区域
    if 所属区域为空:
        从工作地点提取并格式化
    
    # 清洗所属区域数据
    过滤无关词汇
    标准化地址格式
    
    保存到Excel和JSON
```

### 反爬虫策略
- **请求间隔**：页面间延时1秒，职位间延时0.5秒
- **浏览器伪装**：禁用自动化检测特征，模拟真实用户行为
- **验证码处理**：自动检测验证码页面，支持手动处理后继续
- **错误重试**：网络异常自动重试机制，单个失败不影响整体
- **随机化策略**：用户代理轮换，请求头随机化
- **智能延时**：根据响应时间动态调整延时策略

## ⚙️ 配置选项

### 城市配置
在`main()`函数中修改`city_urls`字典来添加或删除城市：
```python
city_urls = {
    "北京": ["https://bj.58.com/hulianwangtx/"],
    "上海": ["https://sh.58.com/hulianwangtx/"],
    # 添加更多城市...
}
```

### 抓取页数
修改`max_pages`参数来调整每个城市的抓取页数：
```python
city_data = scraper.scrape_multiple_pages(base_url, max_pages=5)
```

### 浏览器模式
```python
# 无头模式（后台运行）
scraper = Enhanced58JobScraper(headless=True)

# 可视模式（显示浏览器）
scraper = Enhanced58JobScraper(headless=False)
```

## 📈 性能优化

### Chrome优化选项
```python
# Chrome浏览器优化配置
options = Options()
options.add_argument('--no-sandbox')  # 禁用沙盒模式
options.add_argument('--disable-dev-shm-usage')  # 禁用/dev/shm使用
options.add_argument('--disable-gpu')  # 禁用GPU渲染
options.add_argument('--disable-images')  # 禁用图片加载
options.add_argument('--disable-javascript')  # 禁用JavaScript（可选）
options.add_argument('--disable-plugins')  # 禁用插件
options.add_argument('--disable-extensions')  # 禁用扩展
options.add_argument('--disable-logging')  # 减少日志输出
options.add_argument('--disable-web-security')  # 禁用Web安全检查
options.add_experimental_option('excludeSwitches', ['enable-automation'])  # 禁用自动化检测
options.add_experimental_option('useAutomationExtension', False)  # 禁用自动化扩展
```

### 数据处理优化
- **实时保存策略**：每抓取一个职位立即保存，避免内存溢出和数据丢失
- **智能过滤机制**：在数据保存前进行质量检查，减少无效数据存储
- **批量操作优化**：Excel文件采用追加模式，避免重复读写
- **内存管理**：及时释放不需要的变量，控制内存使用
- **并发控制**：单线程顺序处理，确保数据一致性

### 网络请求优化
- **智能延时**：根据网站响应时间动态调整请求间隔
- **连接复用**：保持WebDriver连接，减少初始化开销
- **超时控制**：设置合理的页面加载超时时间
- **错误恢复**：网络异常时自动重试，提高成功率

## 🚨 注意事项

### 使用限制
1. **遵守robots.txt**：请遵守网站的爬虫协议
2. **合理频率**：避免过于频繁的请求
3. **数据用途**：仅用于学习和研究目的
4. **法律合规**：确保符合相关法律法规

### 常见问题

#### 验证码问题
- **自动检测**：脚本会自动检测验证码页面关键词
- **智能处理**：首先尝试自动刷新页面绕过验证码
- **手动介入**：自动处理失败时暂停并提示手动完成验证
- **继续执行**：验证完成后按回车键继续抓取流程

```python
# 验证码检测逻辑
if "访问过于频繁，本次访问做以下验证码校验" in page_source:
    if self.handle_captcha():
        print("验证码自动处理成功，继续执行...")
    else:
        print("请手动完成验证码验证...")
        input()  # 等待用户按回车
```

#### 数据质量控制
- **必填字段验证**：企业名称、工作职责、任职要求为空的职位会被过滤
- **智能数据补充**：所属区域为空时自动从工作地点提取
- **数据清洗**：过滤包含无关词汇的所属区域信息
- **重复数据处理**：基于企业名称+岗位名称进行去重
- **实时验证**：每个职位保存前都会进行数据完整性检查

#### 网络异常处理
- **分层错误处理**：页面级、职位级、数据级多层异常捕获
- **自动重试机制**：网络超时自动重试，最大重试次数可配置
- **优雅降级**：单个职位失败不影响整体抓取进程
- **状态恢复**：支持中断后从上次位置继续抓取
- **详细日志**：所有异常都会记录到日志文件中

```python
# 错误处理示例
try:
    job_data = self.scrape_job_detail_page(link)
    if job_data:
        self.save_single_job_to_excel(job_data)
except Exception as e:
    print(f"处理第{i}个职位失败: {e}")
    continue  # 继续处理下一个职位
```

#### 性能监控
- **实时进度显示**：显示当前处理的城市、页面、职位序号
- **数据统计**：实时显示已抓取的职位数量
- **速度监控**：显示平均处理速度和预计完成时间
- **内存监控**：监控内存使用情况，防止内存溢出
- **文件大小监控**：实时显示输出文件大小变化

## 📝 更新日志

### v2.1 (当前版本)
- ✅ 完善技术架构文档说明
- ✅ 添加详细的脚本逻辑流程图
- ✅ 增强错误处理和监控功能说明
- ✅ 补充日志系统和数据文件结构说明
- ✅ 优化README文档结构和可读性
- ✅ 添加性能优化配置详解
- ✅ 完善验证码处理机制说明

### v2.0
- ✅ 增强所属区域智能清洗功能
- ✅ 添加更多无关词汇过滤
- ✅ 优化正则表达式匹配
- ✅ 改进数据验证逻辑
- ✅ 修复缩进错误
- ✅ 实现实时数据保存机制
- ✅ 添加智能验证码检测

### v1.0
- ✅ 基础多城市抓取功能
- ✅ 实时数据保存
- ✅ Excel和JSON双格式输出
- ✅ 基础数据清洗
- ✅ Selenium WebDriver集成
- ✅ 基础反爬虫策略

## 🤝 贡献指南

欢迎提交Issue和Pull Request来改进这个项目！

### 开发环境设置
1. Fork本项目
2. 创建功能分支
3. 提交更改
4. 创建Pull Request

## 📄 许可证

本项目仅供学习和研究使用，请勿用于商业目的。

## 📞 联系方式

如有问题或建议，请通过Issue联系。

---

**免责声明**：本工具仅用于技术学习和研究目的，使用者需自行承担使用风险，并确保遵守相关法律法规和网站服务条款。