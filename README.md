# 58同城职位信息爬虫

一个功能强大的58同城招聘信息爬虫工具，支持多城市、多页面批量抓取，具备智能数据清洗和实时保存功能。

## 🚀 功能特性

### 核心功能
- **多城市支持**：支持北京、上海、广州、深圳、成都、西安、郑州等7个主要城市
- **批量抓取**：每个城市自动抓取前5页职位信息
- **实时保存**：每抓取一个职位立即保存到Excel和JSON文件
- **智能去重**：自动过滤重复职位信息
- **数据清洗**：智能清理无效和不规范的数据

### 数据字段
抓取的职位信息包含以下字段：
- 岗位名称
- 企业名称
- 薪资范围（起始/结束）
- 工作地点
- 所属区域（智能清洗后的标准格式）
- 工作职责
- 任职要求
- 学历要求
- 工作经验
- 企业规模
- 企业类型
- 抓取城市
- 抓取时间

### 智能数据处理

#### 所属区域智能清洗
- **格式标准化**：自动提取"XX市XX区"或"XX省XX市XX区"格式
- **无关内容过滤**：过滤包含以下内容的无效数据：
  - 公司相关："有限公司"、"科技有限公司"、"信息科技"
  - 地区描述："华南地区"、"华北地区"、"华东地区"、"华西地区"
  - 招聘信息："找工作"、"免费发布"、"登记简历"
  - 福利信息："公司福利"、"饭补"、"加班补助"、"交通便利"
  - 其他无关："不匹配"、"市中心区"等
- **长度限制**：确保区域名称长度合理（≤10个字符）
- **前缀清理**：自动去除"总部位于"等前缀

#### 薪资信息处理
- 自动解析薪资范围（如"8K-15K"）
- 统一薪资单位（转换为月薪）
- 处理各种薪资表达格式

#### 企业信息标准化
- **企业规模标准化**：统一格式（如"100-499人"）
- **企业类型标准化**：规范行业分类

## 📋 系统要求

### 环境依赖
- Python 3.7+
- Chrome浏览器
- ChromeDriver（自动管理）

### 必需库
```
selenium>=4.0.0
beautifulsoup4>=4.9.0
pandas>=1.3.0
requests>=2.25.0
webdriver-manager>=3.8.0
openpyxl>=3.0.0
```

## 🛠️ 安装与配置

### 1. 克隆项目
```bash
git clone <repository-url>
cd 58同城抓取数据
```

### 2. 创建虚拟环境
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac
```

### 3. 安装依赖
```bash
pip install -r requirements.txt
```

### 4. 运行脚本
```bash
python enhanced_job_scraper.py
```

## 📊 输出文件

### 主要输出
- `58同城多城市职位详细信息.xlsx` - Excel格式的职位数据
- `58同城多城市职位详细信息.json` - JSON格式的职位数据备份

### 辅助工具
- `other/clean_region_enhanced.py` - 数据清洗脚本
- `other/data_comparison.py` - Excel与JSON数据一致性检查
- `other/check_json_count.py` - 数据统计工具

## 🔧 技术架构

### 核心类：Enhanced58JobScraper

#### 主要方法
1. **`__init__(headless=True)`** - 初始化爬虫，配置Chrome选项
2. **`scrape_multiple_pages(base_url, max_pages=5)`** - 批量抓取多页数据
3. **`get_job_list_from_page(url)`** - 抓取单页职位列表
4. **`scrape_job_detail_page(job_url)`** - 抓取职位详情
5. **`scrape_company_detail_page(company_url)`** - 抓取企业详情
6. **`save_single_job_to_excel(job_data, filename)`** - 实时保存单个职位
7. **`save_to_excel(data, filename)`** - 批量保存数据

### 数据处理流程
```
城市URL列表 → 生成页面URL → 抓取职位链接 → 抓取职位详情 → 数据清洗 → 实时保存
```

### 反爬虫策略
- **请求间隔**：页面间延时1秒，职位间延时0.5秒
- **浏览器伪装**：禁用自动化检测特征
- **验证码处理**：自动检测并提示手动处理
- **错误重试**：网络异常自动重试机制

## ⚙️ 配置选项

### 城市配置
在`main()`函数中修改`city_urls`字典来添加或删除城市：
```python
city_urls = {
    "北京": ["https://bj.58.com/hulianwangtx/"],
    "上海": ["https://sh.58.com/hulianwangtx/"],
    # 添加更多城市...
}
```

### 抓取页数
修改`max_pages`参数来调整每个城市的抓取页数：
```python
city_data = scraper.scrape_multiple_pages(base_url, max_pages=5)
```

### 浏览器模式
```python
# 无头模式（后台运行）
scraper = Enhanced58JobScraper(headless=True)

# 可视模式（显示浏览器）
scraper = Enhanced58JobScraper(headless=False)
```

## 📈 性能优化

### Chrome优化选项
- 禁用图片加载
- 禁用GPU渲染
- 禁用插件和扩展
- 减少日志输出
- 优化内存使用

### 数据处理优化
- 实时保存避免内存溢出
- 智能过滤减少无效数据
- 批量操作提高效率

## 🚨 注意事项

### 使用限制
1. **遵守robots.txt**：请遵守网站的爬虫协议
2. **合理频率**：避免过于频繁的请求
3. **数据用途**：仅用于学习和研究目的
4. **法律合规**：确保符合相关法律法规

### 常见问题

#### 验证码问题
- 脚本会自动检测验证码页面
- 出现验证码时会暂停并提示手动处理
- 完成验证后按回车继续执行

#### 数据质量
- 所属区域为空的职位会被跳过
- 企业名称、工作职责、任职要求为空的职位会被过滤
- 重复职位会被自动去重

#### 网络异常
- 脚本具备基本的错误处理机制
- 单个职位失败不会影响整体抓取
- 建议在网络稳定的环境下运行

## 📝 更新日志

### v2.0 (当前版本)
- ✅ 增强所属区域智能清洗功能
- ✅ 添加更多无关词汇过滤
- ✅ 优化正则表达式匹配
- ✅ 改进数据验证逻辑
- ✅ 修复缩进错误

### v1.0
- ✅ 基础多城市抓取功能
- ✅ 实时数据保存
- ✅ Excel和JSON双格式输出
- ✅ 基础数据清洗

## 🤝 贡献指南

欢迎提交Issue和Pull Request来改进这个项目！

### 开发环境设置
1. Fork本项目
2. 创建功能分支
3. 提交更改
4. 创建Pull Request

## 📄 许可证

本项目仅供学习和研究使用，请勿用于商业目的。

## 📞 联系方式

如有问题或建议，请通过Issue联系。

---

**免责声明**：本工具仅用于技术学习和研究目的，使用者需自行承担使用风险，并确保遵守相关法律法规和网站服务条款。